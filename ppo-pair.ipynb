{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ppo for portfolio\n\nimport numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-26T16:58:08.424048Z","iopub.execute_input":"2023-03-26T16:58:08.424652Z","iopub.status.idle":"2023-03-26T16:58:08.478610Z","shell.execute_reply.started":"2023-03-26T16:58:08.424534Z","shell.execute_reply":"2023-03-26T16:58:08.477405Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/cripto-hour/BTCUSDT_norm.csv\n/kaggle/input/cripto-hour/ETHUSDT_norm.csv\n/kaggle/input/cripto-hour/ETHUSDT.csv\n/kaggle/input/cripto-hour/XRPUSDT_norm.csv\n/kaggle/input/cripto-hour/BNBUSDT.csv\n/kaggle/input/cripto-hour/BNBUSDT_norm.csv\n/kaggle/input/cripto-hour/XRPUSDT.csv\n/kaggle/input/cripto-hour/BTCUSDT.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#Environment\n\nfrom re import X\nimport os\nos.environ['PYTHONHASHSEED']=str(1000)\nimport copy\nimport time\nimport json\n#os.system('clear')\nos.system('export MKL_DEBUG_CPU_TYPE=5')\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nfrom collections import deque\nimport numpy as np\nimport os\nos.environ['PYTHONHASHSEED']=str(1000)\nfrom datetime import datetime\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv1D,Conv2D, MaxPooling1D, Activation, Concatenate #, LSTM\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM as LSTM # only for GPU\nfrom tensorflow.keras import backend as K\n\nfrom tensorboardX import SummaryWriter\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n\nrandom.seed(2002)\nnp.random.seed(32)\ntf.random.set_seed(100)\nfrom tensorflow.keras import backend as K\n\n\n\nK.set_image_data_format(\"channels_first\")\n\nclass CustomEnv:\n\t# A custom Bitcoin trading environment\n\tdef __init__(self, df, df_normalized, initial_balance=1000, stocks=['USDCUSDT','BTCUSDT','BNBBTC','BNBBTC'],lookback_window_size=50, model=''):\n\t\t# Define action space and state size and other custom parameters\n\t\t# Normalized Dataset\n\t\tself.xarray = df_normalized\n\t\t# standard dataset, we will use it to calculate prices and quantities\n\t\tself.df = df\n\t\t# Total size of dataset\n\t\tself.df_total_steps = self.xarray.shape[0]\n\t\t# Initial amount of money to invest\n\t\tself.initial_balance = initial_balance\n\t\t# Historical data window\n\t\tself.lookback_window_size = lookback_window_size\n\t\t# Value to normalize transaction data\n\t\tself.normalize_value = 30000\n\t\t#Initial Weights\n\t\tself.weights = [1]+[0]*(self.xarray.shape[2]-1)\n\t\t#Initial quantities\n\t\tself.quants = [0]*self.xarray.shape[2]\n\t\t# Initial quantities for buy and hold\n\t\tself.quants_ubah = [0]*self.xarray.shape[2]\n\t\t# Amout of cash\n\t\tself.cash = 0\n\t\t# list of assets\n\t\tself.stocks =  stocks\n\t\t# Dict for each asset\n\t\tself.market_state = dict.fromkeys(self.stocks)\n\t\t# Model Type\n\t\tself.model = model\n\t\t# Initial amount of money to Buy n hold\n\t\tself.ubah = initial_balance\n\t\t\n\n\t\t# Deque for Order History\n\t\tself.orders_history = deque(maxlen=self.lookback_window_size)\n\t\t\n\t\t\n\t\t# Market history contains the OHCL/Technical Features values for the last lookback_window_size prices\n\t\tself.market_history = deque(maxlen=self.lookback_window_size)\n\n\t\t\n\t# Reset the state of the environment to an initial state\n\tdef reset(self, env_steps_size = 0):\n\t\t\n\t\tself.balance = self.initial_balance\n\t\tself.net_worth = self.initial_balance\n\t\tself.prev_net_worth = self.initial_balance\n\t\tself.weights = [1]+[0]*(self.xarray.shape[2]-1)\n\t\tself.quants = [0]*self.xarray.shape[2]\n\t\tself.quants_ubah = [0]*self.xarray.shape[2]\n\t\tself.short_sell = [1,1,1]\n\t\tself.cash = self.initial_balance\n\t\tself.ubah = self.initial_balance\n\t\t\n\t\t\n\t\t\n\t\t\n\n\n\t\tif env_steps_size > 0: # used for training dataset\n\t\t   \n\t\t   #Randomly selects a value contained between the initial size of the dataset and the final size minus the number of steps.\n\t\t\t\n\t\t\tself.start_step = random.randint(self.lookback_window_size, self.df_total_steps -env_steps_size)\n\t\t\t\n\t\t\tself.end_step = self.start_step + env_steps_size\n\t\telse: # used for testing dataset\n\t\t\t\n\t\t   #Randomly selects a value contained between the initial size of the dataset and the final size minus the number of steps.\n\t\t\tself.start_step = random.randint(self.lookback_window_size, self.df_total_steps -env_steps_size)\n\t\t\t\n\t\t\tself.end_step = self.start_step + env_steps_size\n\t\t\t\n\t\t#Define initial Step\n\t\tself.current_step = self.start_step\n\t\t# Defines the quantities for buy n hold\n\t\tself.quants_ubah = [(self.initial_balance/len(self.weights))/ np.array([self.df[self.current_step,2,x] for x in range(0,len(self.stocks))])]\n\n\t\t# append the data from end t beginning\n\t\tfor i in reversed(range(self.lookback_window_size)):\n\t\t\tcurrent_step = self.current_step - i\n\t\t\tself.orders_history.append([self.net_worth/self.normalize_value,\n\t\t\t\t\t\t\t\t\t\tself.cash/self.normalize_value] +\n\t\t\t\t\t\t\t\t\t\t[number for number in self.quants] +\n\t\t\t\t\t\t\t\t\t\t[number for number in self.weights])\n\n\t\t\n\t\t\n\n\t\t\n\t\t# append the data from end t beginning for each asset\n\t\t\n\t\tfor j in range(0,len(self.stocks)):\n\t\t\t\n\t\t\t\n\t\t\tself.market_state[str(j)] = deque(maxlen=self.lookback_window_size)\n\t\t\t\t \n\t\t\tfor i in reversed(range(self.lookback_window_size)):\n\t\t\t\t\n\t\t\t\tcurrent_step = self.current_step - i\n\n\t\t\t\tself.market_state[str(j)].append(self.xarray[current_step, :,j])\n\t\t\n\t\t\n\t\t#If the model is EIIE type, the state will contain only asset-related data, transaction history data, is added separately in the network.\n\t\tif self.model == \"EIIE\":\n\t\t\tstate = np.stack(([self.market_state[str(x)] for x in range(0,len(self.stocks))]))\n\t\telse:\n\t\t\tstate = np.concatenate(([self.market_state[str(x)] for x in range(0,len(self.stocks))]), axis=1) \n\t\t\tstate = np.concatenate((state, self.orders_history) , axis=1)\n\t\t\n\t\t\n\t\treturn state, self.orders_history\n\n\t# Get the data points for the given current_step\n\tdef _next_observation(self):\n\t\tstart = time.time()\n\t\t\n\t\t\n\t\t# In this step, it updates the state with the most recent point that was used in 'step', for example, in Step it takes the next point after the market history, so if the market history goes to t, in the step it takes the point t+1, in the next observation it appends this point.\n\t\tfor j in range(0, len(self.stocks)):\n\t\t\t\n\t\t\t\n\t\t   \n\t\t\tself.market_state[str(j)].append(self.xarray[self.current_step, :, j])\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t#If the model is EIIE type, the state will contain only asset-related data, transaction history data, is added separately in the network.\n\t\tif self.model == \"EIIE\":\n\t\t\tobs = np.stack(([self.market_state[str(x)] for x in range(0,len(self.stocks))]))\n\t\telse:\n\t\t\tobs = np.concatenate(([self.market_state[str(x)] for x in range(0,len(self.stocks))]), axis=1) \n\t\t\tobs = np.concatenate((obs, self.orders_history) , axis=1)\n\t\t \n\t\treturn obs\n\n\t# Execute one time step within the environment\n\tdef step(self, prediction):\n\t\t# Use to calculate the transactions fee\n\t\tprices_ant =  np.array([self.df[self.current_step,2,x] for x in range(0,len(self.stocks))])\n\t\t# One step on env\n\t\tself.current_step += 1\n\n\n\t\t# Get the prices in the current step\n\t\tprices = np.array([self.df[self.current_step,2,x] for x in range(0,len(self.stocks))])\n\n\n\t\t\n\t\t#Calculates the balance considering the quantities purchased in the previous step, and the prices at the current time\n\t\tself.balance = self.cash + np.dot(prices[1:],self.quants[1:])\n\n\t\t\n\t\t# Use to calculate the transactions fee\n\t\tquants_ant = self.quants\n\n\t\t\n\t\t#Get the quantities, considering the current values and the balance of the previous transaction\n\t\tself.quants = [self.balance*prediction[x]/prices[x] for x in range(0,len(self.stocks))]\n\t\t\n\t\t\n\t\t#Calculate the tax of buying and selling, 10% of the difference between quants of the periods\n\t\t# 0,1% is the binance tax source:\thttps://www.binance.com/en/fee/schedule\n\t\ttax = np.sum(abs(np.dot(np.array(self.quants),prices) - np.dot(np.array(quants_ant),prices_ant)))*0.001\n\n\n\t\t\n\t\t#See the value of the cash term(Stable currency, in the future consider whether this approach is valid)\n\t\tself.cash = self.quants[0]*prices[0]\n\t\t\n \n\t\t\n\t\t\n\t\t#Save the previous net worth\n\t\tself.prev_net_worth = self.net_worth\n\t\t\n\t\t\n\t\t#Calculate the new portfolio value\n\t\tself.net_worth = np.dot(self.quants,prices) - tax\n\t\t\n\t\t\n\t\t#Append the transactions values to deque\n\t\tself.orders_history.append([self.net_worth/self.normalize_value, \n\t\t\t\t\t\t\t\t\tself.cash/self.normalize_value] + \n\t\t\t\t\t\t\t\t\t[number/self.normalize_value for number in self.quants] + prediction.tolist())\n\n\t\t# Calculate reward\n\n\t\t\n\t\treward = np.log(self.net_worth/self.prev_net_worth)\n\t\t#reward = self.net_worth - self.prev_net_worthh\n\t\t\n\t\tif self.net_worth <= self.initial_balance/2:\n\t\t\tdone = True\n\t\telse:\n\t\t\tdone = False\n\t\tobs = self._next_observation() \n\n\t\n\t\treturn obs, self.orders_history, reward, done, prices\n\n\t# render environment\n\tdef render(self):\n\t\tprint(f'Step: {self.current_step}, Net Worth: {self.net_worth}')\n        \n\n        \n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:58:08.592943Z","iopub.execute_input":"2023-03-26T16:58:08.593691Z","iopub.status.idle":"2023-03-26T16:58:19.918471Z","shell.execute_reply.started":"2023-03-26T16:58:08.593634Z","shell.execute_reply":"2023-03-26T16:58:19.916702Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#2789d9;overflow:hidden\">2 | Agent</div>\n","metadata":{}},{"cell_type":"code","source":"\nclass CustomAgent:\n\t# A custom Bitcoin trading agent\n\tdef __init__(self, lookback_window_size=50, lr=0.00004, epochs=4, stocks=[], optimizer=Adam, batch_size=28, model=\"\", shape = [],depth=0, comment=\"\"):\n\t\tself.lookback_window_size = lookback_window_size\n\t\tself.model = model\n\t\tself.comment = comment\n\t\tself.depth = depth\n\t\tself.stocks = stocks\n\t\tself.shape = shape\n\t\t\n\t\t# Action Space it goes from 0 to the number of assets in the portfolio\n\t\tself.action_space = np.array(range(0,len(self.stocks)))\n\n\t\t# folder to save models\n\t\tself.log_name = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")+\"_Crypto_trader\"\n\t\t\n\t\t# State size contains Market+Orders+Indicators history for the last lookback_window_size steps\n\t\tif self.model ==\"EIIE\":\n\t\t\tself.state_size = (len(stocks), lookback_window_size, self.shape[1])\n\t\telse:\n\t\t\tself.state_size = (lookback_window_size, self.shape[1]*self.shape[2]+2+2*self.shape[2]) # 5 standard OHCL information + market and indicators\n\t\t\n\t\t# Neural Networks part\n\t\tself.lr = lr\n\t\tself.epochs = epochs\n\t\tself.optimizer = optimizer\n\t\tself.batch_size = batch_size\n\t\t\n\t\t# Create shared Actor-Critic network model\n\t\tself.Actor = self.Critic = Shared_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer, model=self.model)\n\t\t\n\t# create tensorboard writer\n\tdef create_writer(self, initial_balance, normalize_value, train_episodes):\n\t\tself.replay_count = 0\n\t\tself.writer = SummaryWriter('runs/'+self.log_name)\n\n\t\t# Create folder to save models\n\t\tif not os.path.exists(self.log_name):\n\t\t\tos.makedirs(self.log_name)\n\n\t\tself.start_training_log(initial_balance, normalize_value, train_episodes)\n\t\t\t\n\tdef start_training_log(self, initial_balance, normalize_value, train_episodes):      \n\t\t# save training parameters to Parameters.json file for future\n\t\tcurrent_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n\t\tparams = {\n\t\t\t\"training start\": current_date,\n\t\t\t\"initial balance\": initial_balance,\n\t\t\t\"training episodes\": train_episodes,\n\t\t\t\"lookback window size\": self.lookback_window_size,\n\t\t\t\"depth\": self.depth,\n\t\t\t\"lr\": self.lr,\n\t\t\t\"epochs\": self.epochs,\n\t\t\t\"batch size\": self.batch_size,\n\t\t\t\"normalize value\": normalize_value,\n\t\t\t\"model\": self.model,\n\t\t\t\"comment\": self.comment,\n\t\t\t\"saving time\": \"\",\n\t\t\t\"Actor name\": \"\",\n\t\t\t\"Critic name\": \"\",\n\t\t}\n\t\twith open(self.log_name+\"/Parameters.json\", \"w\") as write_file:\n\t\t\tjson.dump(params, write_file, indent=4)\n\n\n\tdef get_gaes(self, rewards, dones, values, next_values, gamma = 0.98, lamda = 0.94, normalize=True):\n\t\tdeltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n\t\tdeltas = np.stack(deltas)\n\t\tgaes = copy.deepcopy(deltas)\n\t\tfor t in reversed(range(len(deltas) - 1)):\n\t\t\tgaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n\n\t\ttarget = gaes + values\n\t\tif normalize:\n\t\t\tgaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n\t\treturn np.vstack(gaes), np.vstack(target)\n\n\tdef replay(self, states,orders,  rewards, predictions, dones, next_states, orders_history):\n\t\t# reshape memory to appropriate shape for training\n\t\tstates = np.vstack(states)\n\t\torder = np.vstack(orders)\n\t\tnext_states = np.vstack(next_states)\n\t\torders_history =  np.vstack(orders_history)\n\n\t\t\n\t\tpredictions = np.vstack(predictions)\n\n\t\t# Get Critic network predictions\n\t\tif self.model == \"EIIE\":\n\n\t\t\tvalues = self.Critic.critic_predict(states, np.expand_dims(order, axis=1))\n\t\telse:\n\n\t\t\tvalues = self.Critic.critic_predict(states, np.expand_dims(np.expand_dims(order, axis=0), axis=0))\n\n\t\tnext_values = self.Critic.critic_predict(next_states, np.expand_dims(orders_history, axis=1))\n\n\t\t# Compute advantages\n\t\tadvantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n\t\t'''\n\t\tplt.plot(target,'-')\n\t\tplt.plot(advantages,'.')\n\t\tax=plt.gca()\n\t\tax.grid(True)\n\t\tplt.show()\n\t\t'''\n\t\t# stack everything to numpy array\n\t\ty_true = np.hstack([advantages, predictions])\n\t\t\n\n\t\t# training Actor and Critic networks\n\t\tif self.model == \"EIIE\":\n\t\t\ta_loss = self.Actor.Actor.fit([states,np.expand_dims(order, axis=1)], y_true, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n\t\t\tc_loss = self.Critic.Critic.fit([states,np.expand_dims(order, axis=1)], target, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n\t\telse:\n\t\t\ta_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n\t\t\tc_loss = self.Critic.Critic.fit(states, target, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n\t\tself.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n\t\tself.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n\t\tself.replay_count += 1\n\n\t\treturn np.sum(a_loss.history['loss']), np.sum(c_loss.history['loss'])\n\n\tdef act(self, state, order):\n\t\t# Use the network to predict the next action to take, using the model\n\n\t\tprediction = self.Actor.actor_predict(np.expand_dims(state, axis=0), np.expand_dims(np.expand_dims(order, axis=0), axis=0))[0]\n\t\t\n\t\t\n\t\treturn prediction\n\t\t\n\tdef save(self, name=\"Crypto_trader\", score=\"\", args=[]):\n\t\t# save keras model weights\n\t\tself.Actor.Actor.save_weights(f\"{self.log_name}/{score}_{name}_Actor.h5\")\n\t\tself.Critic.Critic.save_weights(f\"{self.log_name}/{score}_{name}_Critic.h5\")\n\n\t\t# update json file settings\n\t\tif score != \"\":\n\t\t\twith open(self.log_name+\"/Parameters.json\", \"r\") as json_file:\n\t\t\t\tparams = json.load(json_file)\n\t\t\tparams[\"saving time\"] = datetime.now().strftime('%Y-%m-%d %H:%M')\n\t\t\tparams[\"Actor name\"] = f\"{score}_{name}_Actor.h5\"\n\t\t\tparams[\"Critic name\"] = f\"{score}_{name}_Critic.h5\"\n\t\t\twith open(self.log_name+\"/Parameters.json\", \"w\") as write_file:\n\t\t\t\tjson.dump(params, write_file, indent=4)\n\n\t\t# log saved model arguments to file\n\t\tif len(args) > 0:\n\t\t\twith open(f\"{self.log_name}/log.txt\", \"a+\") as log:\n\t\t\t\tcurrent_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\t\t\t\targuments = \"\"\n\t\t\t\tfor arg in args:\n\t\t\t\t\targuments += f\", {arg}\"\n\t\t\t\tlog.write(f\"{current_time}{arguments}\\n\")\n\n\tdef load(self, folder, name):\n\t\t# load keras model weights\n\t\tself.Actor.Actor.load_weights(os.path.join(folder, f\"{name}_Actor.h5\"))\n\t\tself.Critic.Critic.load_weights(os.path.join(folder, f\"{name}_Critic.h5\"))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:58:19.921127Z","iopub.execute_input":"2023-03-26T16:58:19.921786Z","iopub.status.idle":"2023-03-26T16:58:19.963407Z","shell.execute_reply.started":"2023-03-26T16:58:19.921750Z","shell.execute_reply":"2023-03-26T16:58:19.962184Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#2789d9;overflow:hidden\">3 | Model</div>\n","metadata":{}},{"cell_type":"code","source":"\n#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\ntf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n\nnp.random.seed(32)\ntf.random.set_seed(100)\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif len(gpus) > 0:\n\tprint(f'GPUs {gpus}')\n\ttry: tf.config.experimental.set_memory_growth(gpus[0], True)\n\texcept RuntimeError: pass\n\nclass Shared_Model:\n\tdef __init__(self, input_shape, action_space, lr, optimizer, model=\"Dense\"):\n\t\tX_input = Input(input_shape)\n\t\tself.action_space = action_space\n\t\t\n\t\tself.model = model\n\n\t\t# Shared CNN layers:\n\t\tif model==\"CNN\":\n\t\t\tX = Dense(filters=64, kernel_size=6, padding=\"same\", activation=\"swish\")(X_input)\n\t\t\tX = MaxPooling1D(pool_size=2)(X)\n\t\t\tX = Dense(filters=32, kernel_size=3, padding=\"same\", activation=\"swish\")(X)\n\t\t\tX = MaxPooling1D(pool_size=2)(X)\n\t\t\tX = Flatten()(X)\n\t\t#EIIE Layers\n\t\telif model==\"EIIE\":\n\t\t\tX = Conv2D(2, (3, 1))(X_input)\n\t\t\tX = Activation('tanh')(X)\n\t\t\tX = Conv2D(20, (48, 1))(X)\n\t\t\tX = Activation('tanh')(X)\n\t\t\t\n\t\t\tinputB = Input(shape=(1, 50, 10))\n\t\t\tmodelB = Conv2D(filters=2, kernel_size=(3, 1), activation='tanh')(inputB)\n\t\t\tmodelB = Conv2D(filters=20, kernel_size=(50 - 2, 1), activation='tanh')(modelB)\n\t\t\tmerged = Concatenate(axis=3)([X, modelB])\n\t\t\tX = Conv2D(filters=1, kernel_size=(1, 1))(merged)\n\n\t\t\t#output = Dense(self.action_space, activation=\"softmax\")(x)\n\t\t# Shared LSTM layers:\n\t\telif model==\"LSTM\":\n\t\t\tX = LSTM(512, return_sequences=True)(X_input)\n\t\t\tX = LSTM(256)(X)\n\n\t\t# Shared Dense layers:\n\t\telse:\n\t\t\tX = Flatten()(X_input)\n\t\t\tX = Dense(512, activation=\"softmax\")(X)\n\t\t\n\t\t# Critic model\n\t\tV = Dense(512, activation=\"selu\")(X)\n\t\tV = Dense(256, activation=\"selu\")(V)\n\t\tV = Dense(64, activation=\"selu\")(V)\n\t\tvalue = Dense(1, activation=None)(V)\n\t\tif model == \"EIIE\":\n\t\t\tself.Critic = Model(inputs=[X_input,inputB], outputs = value)\n\t\telse:\n\t\t\tself.Critic = Model(inputs=X_input, outputs = value)\n\t\tself.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(lr=lr))\n\n\t\t# Actor model\n\t\tA = Dense(512, activation=\"sigmoid\")(X)\n\t\tA = Dense(256, activation=\"sigmoid\")(A)\n\t\tA = Dense(64, activation=\"sigmoid\")(A)\n\t\toutput = Dense(self.action_space, activation=\"softmax\")(A)\n\t\tif model == \"EIIE\":\n\t\t\tself.Actor = Model(inputs = [X_input,inputB], outputs = output)\n\t\telse:\n\t\t\tself.Actor = Model(inputs = X_input, outputs = output)\n\t\tself.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n\n\tdef ppo_loss(self, y_true, y_pred):\n\t\t\n\t\tadvantages, prediction_picks = y_true[:, :1], y_true[:, 1:1+self.action_space]\n\t\tLOSS_CLIPPING = 0.22\n\t\tENTROPY_LOSS = 0.002\n\t\t# Calculate the ratio pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)\n\t\t# NOTE: we just subtract the logs, which is the same as\n\t\t# dividing the values and then canceling the log with e^log.\n\t\t# For why we use log probabilities instead of actual probabilities,\n\t\t\n\t\tprob = y_pred\n\t\told_prob = prediction_picks\n\n\t\tprob = K.clip(prob, 1e-10, 1.0)\n\t\told_prob = K.clip(old_prob, 1e-10, 1.0)\n\n\t\tratio = K.exp(K.log(prob) - K.log(old_prob))\n\t\t\n\t\tp1 = ratio * advantages\n\t\tp2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n\n\t\tactor_loss = -K.mean(K.minimum(p1, p2))\n\n\t\tentropy = -(y_pred * K.log(y_pred + 1e-10))\n\t\tentropy = ENTROPY_LOSS * K.mean(entropy)\n\t\t\n\t\ttotal_loss = actor_loss - entropy\n\n\t\treturn total_loss\n\n\tdef actor_predict(self, state, order):\n\t\tif self.model == \"EIIE\":\n\t\t\t\n\t\t\treturn self.Actor.predict([state, order])\n\t\telse:\n\t\t\t\n\t\t\treturn self.Actor.predict([state, np.zeros((state.shape[0], 1))])\n\t\t\n\tdef critic_PPO2_loss(self, y_true, y_pred):\n\t\tvalue_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n\t\treturn value_loss\n\n\tdef critic_predict(self, state, order):\n\t\tif self.model == \"EIIE\":\n\t\t\t\n\t\t\treturn self.Critic.predict([state, order])\n\t\telse:\n\t\t\t\n\t\t\treturn self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n\n\t\t\nclass Actor_Model:\n\tdef __init__(self, input_shape, action_space, lr, optimizer):\n\t\tX_input = Input(input_shape)\n\t\tself.action_space = action_space\n\n\t\tX = Flatten(input_shape=input_shape)(X_input)\n\t\tX = Concatenate(512, activation=\"sigmoid\")(X)\n\t\tX = Concatenate(256, activation=\"sigmoid\")(X)\n\t\tX = Concatenate(64, activation=\"sigmoid\")(X)\n\t\toutput = Dense(self.action_space, activation=\"softmax\")(X)\n\n\t\tself.Actor = Model(inputs = X_input, outputs = output)\n\t\tself.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n\t\t\n\n\tdef ppo_loss(self, y_true, y_pred):\n\t\tadvantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n\t\tLOSS_CLIPPING = 0.22\n\t\tENTROPY_LOSS = 0.002\n\t\t\n\t\tprob = actions * y_pred\n\t\told_prob = actions * prediction_picks\n\n\t\tprob = K.clip(prob, 1e-10, 1.0)\n\t\told_prob = K.clip(old_prob, 1e-10, 1.0)\n\n\t\tratio = K.exp(K.log(prob) - K.log(old_prob))\n\t\t\n\t\tp1 = ratio * advantages\n\t\tp2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n\n\t\tactor_loss = -K.mean(K.minimum(p1, p2))\n\n\t\tentropy = -(y_pred * K.log(y_pred + 1e-10))\n\t\tentropy = ENTROPY_LOSS * K.mean(entropy)\n\t\t\n\t\ttotal_loss = actor_loss - entropy\n\n\t\treturn total_loss\n\n\tdef actor_predict(self, state):\n\t\treturn self.Actor.predict(state)\n\nclass Critic_Model:\n\tdef __init__(self, input_shape, action_space, lr, optimizer):\n\t\tX_input = Input(input_shape)\n\n\t\tV = Flatten(input_shape=input_shape)(X_input)\n\t\tV = Dense(512, activation=\"tanh\")(V)\n\t\tV = Dense(256, activation=\"tanh\")(V)\n\t\tV = Dense(64, activation=\"tanh\")(V)\n\t\tvalue = Dense(1, activation=\"softmax\")(V)\n\n\t\tself.Critic = Model(inputs=X_input, outputs = value)\n\t\tself.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(lr=lr))\n\n\tdef critic_PPO2_loss(self, y_true, y_pred):\n\t\tvalue_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n\t\treturn value_loss\n\n\tdef critic_predict(self, state):\n\t\treturn self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:58:19.964712Z","iopub.execute_input":"2023-03-26T16:58:19.965109Z","iopub.status.idle":"2023-03-26T16:58:20.041379Z","shell.execute_reply.started":"2023-03-26T16:58:19.965051Z","shell.execute_reply":"2023-03-26T16:58:20.040190Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#2789d9;overflow:hidden\">4 | Train</div>","metadata":{}},{"cell_type":"code","source":"def train_agent(env, agent, visualize=False, train_episodes = 49, training_batch_size=490):\n\tagent.create_writer(env.initial_balance, env.normalize_value, train_episodes) # create TensorBoard writer\n\ttotal_average = deque(maxlen=100) # save recent 100 episodes net worth\n\tbest_average = 0 # used to track best average net worth\n\n\tfor episode in range(train_episodes):\n\t\t# Reset the env\n\t\tstate, order = env.reset(env_steps_size = training_batch_size)\n\t\tstates, orders, rewards, predictions, dones, next_states, next_orders = [], [], [], [], [], [], []\n\t\tfor t in range(training_batch_size):\n\t\t\t# Gets the action to be taken by the agent\n\t\t\tprediction = agent.act(state, np.array(order))\n\t\t\t\n\t\t\t\n\t\t\tprediction = np.squeeze(prediction)\n\n\t\t\t# Perform an action on env\n\t\t\tnext_state, next_order, reward, done, prices = env.step( prediction)\n\t\t\tstates.append(np.expand_dims(state, axis=0))\n\t\t\torders.append(np.expand_dims(order, axis=0))\n\t\t\t\n\t\t\tnext_states.append(np.expand_dims(next_state, axis=0))\n\t\t\tnext_orders.append(np.expand_dims(next_order, axis=0))\n\n\t\t\trewards.append(reward)\n\t\t\tdones.append(done)\n\t\t\tpredictions.append(prediction)\n\t\t\tstate = next_state\n\t\t\torder = next_order\n\n\t\t# Train the Model\n\t\ta_loss, c_loss = agent.replay(states, orders, rewards, predictions, dones, next_states, next_orders)\n\t\ttotal_average.append(env.net_worth)\n\t\taverage = np.average(total_average)\n\t\trewardFull = np.average(rewards)\n\n\t\tagent.writer.add_scalar('Data/average reward', rewardFull, episode)\n\t\tagent.writer.add_scalar('Data/average net_worth', average, episode)\n\t\tagent.writer.add_scalar('Data/average net_worth_percent',  round((average-1000)/10), episode)\n\t\t\n\t\t\n\t\tprint(\"net worth {} {:.2f} {:.2f} {:.2f} % UBAH {:.2f} diff {:.2f}\".format(episode, env.net_worth, average,  (average-1000)/10, float(np.dot(env.quants_ubah,prices)), env.net_worth - float(np.dot(env.quants_ubah,prices)) ))\n\t\tif episode > len(total_average):\n\t\t\tif best_average < average:\n\t\t\t\tbest_average = average\n\t\t\t\tprint(\"Saving model\")\n\t\t\t\tagent.save(score=\"{:.2f}\".format(best_average), args=[episode, average,  a_loss, c_loss])\n\t\t\tagent.save()\n\t\n\ndef test_agent(env, visualize=True, test_episodes=10,testing_batch_size=500):\n\t# load the model\n\t# env.load()\n\t\n\taverage_net_worth = 0\n\taverage_UBAH = 0\n\tfor episode in range(test_episodes):\n\t\tstate, order = env.reset(env_steps_size = testing_batch_size)\n\t\told= 0\n\t\tfor t in range(testing_batch_size):\n\t\t\tprediction = agent.act(state, np.array(order))\n\t\t\tprediction = np.squeeze(prediction)\n\t\t\told = env.net_worth\n\t\t\tstate, order, reward, done, prices = env.step(prediction)\n\t\t\t\n\n\t\t\n\t\taverage_net_worth += env.net_worth\n\t\taverage_UBAH += np.dot(env.quants_ubah,prices)\n\t\tprint(\"net_worth:{:.2f} % {:.2f} UBAH {:.2f} diff {:.2f}\".format(env.net_worth,(env.net_worth-1000)/10, float(np.dot(env.quants_ubah,prices)), env.net_worth - float(np.dot(env.quants_ubah,prices))))\n\t\t\n\t\t\t\n\tprint(\"average {:.2f} % {:.2f}\".format(test_episodes, average_net_worth/test_episodes), average_UBAH/test_episodes)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:58:20.043820Z","iopub.execute_input":"2023-03-26T16:58:20.044576Z","iopub.status.idle":"2023-03-26T16:58:20.065298Z","shell.execute_reply.started":"2023-03-26T16:58:20.044527Z","shell.execute_reply":"2023-03-26T16:58:20.064214Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"ETHUSDT = pd.read_csv('../input/cripto-hour/ETHUSDT.csv')\nBTCUSDT = pd.read_csv('../input/cripto-hour/BTCUSDT.csv')\nBNBUSDT = pd.read_csv('../input/cripto-hour/BNBUSDT.csv')\nXRPUSDT = pd.read_csv('../input/cripto-hour/XRPUSDT.csv')\nETHUSDT_norm = pd.read_csv('../input/cripto-hour/ETHUSDT_norm.csv')\nBTCUSDT_norm = pd.read_csv('../input/cripto-hour/BTCUSDT_norm.csv')\nBNBUSDT_norm = pd.read_csv('../input/cripto-hour/BNBUSDT_norm.csv')\nXRPUSDT_norm = pd.read_csv('../input/cripto-hour/XRPUSDT_norm.csv')\n\ntrain_df = []\ntrain_df_norm = []\ntrain_df.append(ETHUSDT.values)\ntrain_df.append(BTCUSDT.values)\ntrain_df.append(BNBUSDT.values)\ntrain_df.append(XRPUSDT.values)\ntrain_df_norm.append(XRPUSDT_norm.values)\ntrain_df_norm.append(BTCUSDT_norm.values)\ntrain_df_norm.append(BNBUSDT_norm.values)\ntrain_df_norm.append(XRPUSDT_norm.values)\n\nxa = np.copy(np.moveaxis(np.array(train_df),0,-1))\nx_norm = np.copy(np.moveaxis(np.array(train_df_norm),0,-1))\nlookback_window_size = 50\nprint('shapedp',x_norm.shape, xa.shape)\ntrain_df = xa[:-10000-lookback_window_size]\ntrain_df_norm = x_norm[:-10000-lookback_window_size]\ntest_df = xa[-10000:] # 30 days\ntest_df_norm = x_norm[-10000:]\n\n\n\nprint('shape12345',train_df.shape)\nmodel = 'EIIE'\ntrain_env = CustomEnv(train_df, train_df_norm, lookback_window_size=lookback_window_size, model=model)\ntest_env = CustomEnv(test_df, test_df_norm,lookback_window_size=lookback_window_size, model=model)\nagent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00004, epochs=4, stocks=['BNBUSDT','BNBUSDT','BNBUSDT','XRPUSDT'], optimizer=Adam, batch_size = 28, model=model, shape = x_norm.shape)\ntrain_agent(train_env, agent, visualize=False, train_episodes=49, training_batch_size=490)\ntest_agent(test_env, visualize=False, test_episodes=10, testing_batch_size=500)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T16:58:20.066552Z","iopub.execute_input":"2023-03-26T16:58:20.067095Z","iopub.status.idle":"2023-03-26T17:01:53.810249Z","shell.execute_reply.started":"2023-03-26T16:58:20.067041Z","shell.execute_reply":"2023-03-26T17:01:53.808895Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"shapedp (35511, 14, 4) (35512, 14, 4)\nshape12345 (25462, 14, 4)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  warnings.warn('`Model.state_updates` will be removed in a future version. '\n","output_type":"stream"},{"name":"stdout","text":"net worth 0 1873.92 1873.92 87.39 % UBAH 2810.00 diff -936.08\nnet worth 1 893.24 1383.58 38.36 % UBAH 869.27 diff 23.98\nnet worth 2 1102.03 1289.73 28.97 % UBAH 1123.63 diff -21.59\nnet worth 3 942.83 1203.01 20.30 % UBAH 947.72 diff -4.89\nnet worth 4 1108.96 1184.20 18.42 % UBAH 1151.96 diff -43.00\nnet worth 5 1046.87 1161.31 16.13 % UBAH 1047.39 diff -0.52\nnet worth 6 1051.35 1145.60 14.56 % UBAH 1031.86 diff 19.49\nnet worth 7 967.43 1123.33 12.33 % UBAH 985.40 diff -17.97\nnet worth 8 1025.60 1112.47 11.25 % UBAH 1092.71 diff -67.11\nnet worth 9 910.98 1092.32 9.23 % UBAH 933.55 diff -22.57\nnet worth 10 1113.83 1094.28 9.43 % UBAH 1097.36 diff 16.47\nnet worth 11 969.48 1083.88 8.39 % UBAH 947.83 diff 21.65\nnet worth 12 1067.02 1082.58 8.26 % UBAH 1028.72 diff 38.30\nnet worth 13 1282.23 1096.84 9.68 % UBAH 1355.56 diff -73.33\nnet worth 14 1172.03 1101.85 10.19 % UBAH 1264.10 diff -92.07\nnet worth 15 851.29 1086.19 8.62 % UBAH 802.74 diff 48.55\nnet worth 16 895.66 1074.99 7.50 % UBAH 852.07 diff 43.59\nnet worth 17 790.60 1059.19 5.92 % UBAH 780.73 diff 9.87\nnet worth 18 961.35 1054.04 5.40 % UBAH 931.74 diff 29.61\nnet worth 19 899.76 1046.32 4.63 % UBAH 862.19 diff 37.57\nnet worth 20 911.84 1039.92 3.99 % UBAH 871.86 diff 39.98\nnet worth 21 950.49 1035.86 3.59 % UBAH 944.01 diff 6.48\nnet worth 22 824.71 1026.68 2.67 % UBAH 818.54 diff 6.17\nnet worth 23 942.42 1023.16 2.32 % UBAH 967.17 diff -24.75\nnet worth 24 945.63 1020.06 2.01 % UBAH 919.02 diff 26.60\nnet worth 25 965.93 1017.98 1.80 % UBAH 968.20 diff -2.27\nnet worth 26 786.27 1009.40 0.94 % UBAH 759.21 diff 27.05\nnet worth 27 987.38 1008.61 0.86 % UBAH 942.16 diff 45.22\nnet worth 28 880.79 1004.20 0.42 % UBAH 871.87 diff 8.92\nnet worth 29 1178.71 1010.02 1.00 % UBAH 1228.93 diff -50.21\nnet worth 30 1043.28 1011.09 1.11 % UBAH 1029.95 diff 13.33\nnet worth 31 1148.26 1015.38 1.54 % UBAH 1224.05 diff -75.79\nnet worth 32 1034.59 1015.96 1.60 % UBAH 1057.43 diff -22.84\nnet worth 33 787.88 1009.25 0.93 % UBAH 748.43 diff 39.45\nnet worth 34 1207.94 1014.93 1.49 % UBAH 1268.62 diff -60.68\nnet worth 35 1029.57 1015.34 1.53 % UBAH 1032.23 diff -2.66\nnet worth 36 1143.35 1018.80 1.88 % UBAH 1198.06 diff -54.72\nnet worth 37 987.65 1017.98 1.80 % UBAH 1033.84 diff -46.19\nnet worth 38 956.83 1016.41 1.64 % UBAH 1004.01 diff -47.18\nnet worth 39 1322.43 1024.06 2.41 % UBAH 1406.34 diff -83.91\nnet worth 40 1013.48 1023.80 2.38 % UBAH 1023.44 diff -9.96\nnet worth 41 1391.99 1032.57 3.26 % UBAH 1962.97 diff -570.98\nnet worth 42 943.29 1030.49 3.05 % UBAH 943.55 diff -0.26\nnet worth 43 1273.67 1036.02 3.60 % UBAH 1444.02 diff -170.35\nnet worth 44 985.33 1034.89 3.49 % UBAH 1003.55 diff -18.22\nnet worth 45 768.49 1029.10 2.91 % UBAH 742.82 diff 25.67\nnet worth 46 1300.84 1034.88 3.49 % UBAH 1402.65 diff -101.82\nnet worth 47 936.63 1032.84 3.28 % UBAH 912.13 diff 24.50\nnet worth 48 1261.66 1037.51 3.75 % UBAH 1389.91 diff -128.26\nnet_worth:895.07 % -10.49 UBAH 855.50 diff 39.57\nnet_worth:990.09 % -0.99 UBAH 954.11 diff 35.99\nnet_worth:1017.11 % 1.71 UBAH 962.33 diff 54.79\nnet_worth:655.59 % -34.44 UBAH 598.14 diff 57.46\nnet_worth:951.50 % -4.85 UBAH 966.11 diff -14.61\nnet_worth:1074.97 % 7.50 UBAH 1145.55 diff -70.57\nnet_worth:766.23 % -23.38 UBAH 690.72 diff 75.51\nnet_worth:975.33 % -2.47 UBAH 943.90 diff 31.43\nnet_worth:1019.24 % 1.92 UBAH 997.58 diff 21.67\nnet_worth:1056.92 % 5.69 UBAH 1015.80 diff 41.11\naverage 10.00 % 940.21 [912.97259444]\n","output_type":"stream"}]}]}